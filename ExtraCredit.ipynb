{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce55F2Onn2v",
        "outputId": "2ba322ce-0210-4088-f346-7aa15f9a58c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing training data.\n",
            "category                 int64\n",
            "amt                    float64\n",
            "lat                    float64\n",
            "long                   float64\n",
            "city_pop                 int64\n",
            "                        ...   \n",
            "state_amt_to_mean      float64\n",
            "state_amt_to_median    float64\n",
            "state_distance_mean    float64\n",
            "state_distance_std     float64\n",
            "state_tx_count           int64\n",
            "Length: 62, dtype: object\n",
            "\n",
            "Total number of features used 62\n",
            "\n",
            " training data.\n",
            "[0]\ttrain-auc:0.98228\tval-auc:0.98177\n",
            "[100]\ttrain-auc:0.99912\tval-auc:0.99897\n",
            "[200]\ttrain-auc:0.99982\tval-auc:0.99944\n",
            "[300]\ttrain-auc:0.99994\tval-auc:0.99946\n",
            "[400]\ttrain-auc:0.99998\tval-auc:0.99945\n",
            "[500]\ttrain-auc:0.99999\tval-auc:0.99945\n",
            "[600]\ttrain-auc:1.00000\tval-auc:0.99944\n",
            "[700]\ttrain-auc:1.00000\tval-auc:0.99943\n",
            "[800]\ttrain-auc:1.00000\tval-auc:0.99940\n",
            "[900]\ttrain-auc:1.00000\tval-auc:0.99941\n",
            "[999]\ttrain-auc:1.00000\tval-auc:0.99940\n",
            "\n",
            "Model Evaluation:\n",
            "F1 Score: 0.9775407407407407\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     65681\n",
            "           1       0.98      0.97      0.98      8460\n",
            "\n",
            "    accuracy                           0.99     74141\n",
            "   macro avg       0.99      0.99      0.99     74141\n",
            "weighted avg       0.99      0.99      0.99     74141\n",
            "\n",
            "\n",
            "Processing test data\n",
            "category                 int64\n",
            "amt                    float64\n",
            "lat                    float64\n",
            "long                   float64\n",
            "city_pop                 int64\n",
            "                        ...   \n",
            "state_amt_to_mean      float64\n",
            "state_amt_to_median    float64\n",
            "state_distance_mean    float64\n",
            "state_distance_std     float64\n",
            "state_tx_count           int64\n",
            "Length: 62, dtype: object\n",
            "\n",
            "Total number of features used 62\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def preprocess_data(df, label_encoders=None, is_training=True):\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Process dates\n",
        "    df_processed['trans_date'] = pd.to_datetime(df_processed['trans_date'])\n",
        "    df_processed['trans_time'] = pd.to_datetime(df_processed['trans_time'])\n",
        "    df_processed['dob'] = pd.to_datetime(df_processed['dob'])\n",
        "    df_processed['hour'] = df_processed['trans_time'].dt.hour\n",
        "    df_processed['day'] = df_processed['trans_date'].dt.day\n",
        "    df_processed['month'] = df_processed['trans_date'].dt.month\n",
        "    df_processed['weekday'] = df_processed['trans_date'].dt.weekday\n",
        "    df_processed['year'] = df_processed['trans_date'].dt.year\n",
        "\n",
        "    # Time-based features\n",
        "    df_processed['is_weekend'] = df_processed['weekday'].isin([5, 6]).astype(int)\n",
        "    df_processed['is_night'] = ((df_processed['hour'] >= 22) | (df_processed['hour'] <= 5)).astype(int)\n",
        "    df_processed['is_morning'] = ((df_processed['hour'] >= 6) & (df_processed['hour'] <= 11)).astype(int)\n",
        "\n",
        "    # Calculate distance using the given lat and long\n",
        "    df_processed['distance'] = np.sqrt(\n",
        "        (df_processed['lat'] - df_processed['merch_lat'])**2 +\n",
        "        (df_processed['long'] - df_processed['merch_long'])**2\n",
        "    )\n",
        "\n",
        "    # Age\n",
        "    df_processed['age'] = (df_processed['trans_date'] - df_processed['dob']).dt.days / 365.25\n",
        "\n",
        "    # Amount\n",
        "    df_processed['amount_log'] = np.log1p(df_processed['amt'])\n",
        "\n",
        "    # Population density\n",
        "    df_processed['city_pop_log'] = np.log1p(df_processed['city_pop'])\n",
        "\n",
        "    # Drop the not needed columns\n",
        "    columns_to_drop = ['trans_date', 'trans_time', 'cc_num', 'first', 'last',\n",
        "                      'street', 'city', 'dob', 'unix_time', 'zip', 'trans_num']\n",
        "    df_processed = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Categorical encoding\n",
        "    cat_columns = ['category', 'gender', 'state', 'job', 'merchant']\n",
        "\n",
        "    if is_training:\n",
        "        label_encoders = {}\n",
        "        for col in cat_columns:\n",
        "            label_encoders[col] = LabelEncoder()\n",
        "            df_processed[col] = label_encoders[col].fit_transform(df_processed[col].astype(str))\n",
        "        return df_processed, label_encoders\n",
        "    else:\n",
        "        for col in cat_columns:\n",
        "            df_processed[col] = df_processed[col].astype(str)\n",
        "            unseen = ~df_processed[col].isin(label_encoders[col].classes_)\n",
        "            df_processed.loc[unseen, col] = label_encoders[col].classes_[0]\n",
        "            df_processed[col] = label_encoders[col].transform(df_processed[col])\n",
        "        return df_processed\n",
        "\n",
        "def prepare_features(df_processed, is_training=True, agg_stats=None):\n",
        "\n",
        "    feature_columns = [\n",
        "        'category', 'amt', 'lat', 'long', 'city_pop', 'job',\n",
        "        'merch_lat', 'merch_long', 'hour', 'day', 'month', 'weekday',\n",
        "        'is_weekend', 'is_night', 'amount_log', 'distance', 'age',\n",
        "        'gender', 'state', 'merchant'\n",
        "    ]\n",
        "\n",
        "    df_features = df_processed[feature_columns].copy()\n",
        "\n",
        "    df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour']/24)\n",
        "    df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour']/24)\n",
        "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day']/31)\n",
        "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day']/31)\n",
        "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month']/12)\n",
        "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month']/12)\n",
        "    df_features['weekday_sin'] = np.sin(2 * np.pi * df_features['weekday']/7)\n",
        "    df_features['weekday_cos'] = np.cos(2 * np.pi * df_features['weekday']/7)\n",
        "\n",
        "    # Time features\n",
        "    df_features['is_late_night'] = ((df_features['hour'] >= 23) | (df_features['hour'] <= 4)).astype(int)\n",
        "    df_features['is_business_hour'] = ((df_features['hour'] >= 9) & (df_features['hour'] <= 17)).astype(int)\n",
        "    df_features['is_evening'] = ((df_features['hour'] >= 18) & (df_features['hour'] <= 22)).astype(int)\n",
        "\n",
        "    # Amount features\n",
        "    df_features['amount_sqrt'] = np.sqrt(df_features['amt'])\n",
        "    df_features['amount_squared'] = df_features['amt'] ** 2\n",
        "\n",
        "    # location features\n",
        "    df_features['distance_log'] = np.log1p(df_features['distance'])\n",
        "    df_features['city_pop_log'] = np.log1p(df_features['city_pop'])\n",
        "\n",
        "    # others\n",
        "    df_features['amount_per_distance'] = df_features['amt'] / (df_features['distance'] + 1)\n",
        "    df_features['amount_per_pop'] = df_features['amt'] / (df_features['city_pop'] + 1)\n",
        "    df_features['pop_per_distance'] = df_features['city_pop'] / (df_features['distance'] + 1)\n",
        "    if is_training:\n",
        "        agg_stats = {}\n",
        "        for col in ['category', 'merchant', 'state']:\n",
        "            agg_stats[f'{col}_amt_mean'] = df_features.groupby(col)['amt'].mean()\n",
        "            agg_stats[f'{col}_amt_std'] = df_features.groupby(col)['amt'].std()\n",
        "            agg_stats[f'{col}_amt_median'] = df_features.groupby(col)['amt'].median()\n",
        "            agg_stats[f'{col}_distance_mean'] = df_features.groupby(col)['distance'].mean()\n",
        "            agg_stats[f'{col}_distance_std'] = df_features.groupby(col)['distance'].std()\n",
        "            agg_stats[f'{col}_tx_count'] = df_features.groupby(col)['amt'].count()\n",
        "\n",
        "            df_features[f'{col}_amt_mean'] = df_features[col].map(agg_stats[f'{col}_amt_mean'])\n",
        "            df_features[f'{col}_amt_std'] = df_features[col].map(agg_stats[f'{col}_amt_std'])\n",
        "            df_features[f'{col}_amt_median'] = df_features[col].map(agg_stats[f'{col}_amt_median'])\n",
        "            df_features[f'{col}_amt_to_mean'] = df_features['amt'] / (df_features[f'{col}_amt_mean'] + 1)\n",
        "            df_features[f'{col}_amt_to_median'] = df_features['amt'] / (df_features[f'{col}_amt_median'] + 1)\n",
        "            df_features[f'{col}_distance_mean'] = df_features[col].map(agg_stats[f'{col}_distance_mean'])\n",
        "            df_features[f'{col}_distance_std'] = df_features[col].map(agg_stats[f'{col}_distance_std'])\n",
        "            df_features[f'{col}_tx_count'] = df_features[col].map(agg_stats[f'{col}_tx_count'])\n",
        "    else:\n",
        "        for col in ['category', 'merchant', 'state']:\n",
        "            df_features[f'{col}_amt_mean'] = df_features[col].map(agg_stats[f'{col}_amt_mean'])\n",
        "            df_features[f'{col}_amt_std'] = df_features[col].map(agg_stats[f'{col}_amt_std'])\n",
        "            df_features[f'{col}_amt_median'] = df_features[col].map(agg_stats[f'{col}_amt_median'])\n",
        "            df_features[f'{col}_amt_to_mean'] = df_features['amt'] / (df_features[f'{col}_amt_mean'] + 1)\n",
        "            df_features[f'{col}_amt_to_median'] = df_features['amt'] / (df_features[f'{col}_amt_median'] + 1)\n",
        "            df_features[f'{col}_distance_mean'] = df_features[col].map(agg_stats[f'{col}_distance_mean'])\n",
        "            df_features[f'{col}_distance_std'] = df_features[col].map(agg_stats[f'{col}_distance_std'])\n",
        "            df_features[f'{col}_tx_count'] = df_features[col].map(agg_stats[f'{col}_tx_count'])\n",
        "\n",
        "    for col in df_features.columns: # simple data processing\n",
        "        df_features[col] = df_features[col].replace([np.inf, -np.inf], 0)\n",
        "        df_features[col] = df_features[col].fillna(0)\n",
        "\n",
        "    print(df_features.dtypes)\n",
        "    print(f\"\\nTotal number of features used {len(df_features.columns)}\")\n",
        "\n",
        "    if is_training:\n",
        "        target = df_processed['is_fraud']\n",
        "        return df_features, target, agg_stats\n",
        "\n",
        "    return df_features\n",
        "\n",
        "def make_predictions_and_submit(model, X_test, test_ids, output_path):\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "    y_pred = model.predict(dtest)\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'is_fraud': (y_pred > 0.5).astype(int)\n",
        "    })\n",
        "    submission.to_csv(output_path, index=False)\n",
        "    return submission\n",
        "\n",
        "def main():\n",
        "    train_df = pd.read_csv('/content/train (2).csv')\n",
        "    test_df = pd.read_csv('/content/test (2).csv')\n",
        "\n",
        "    print(\"\\nPreprocessing training data.\")\n",
        "    df_processed, label_encoders = preprocess_data(train_df, is_training=True)\n",
        "    X, y, agg_stats = prepare_features(df_processed, is_training=True)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
        "\n",
        "    tree_params = { #tune these parameter\n",
        "        'objective': 'binary:logistic',\n",
        "        'booster': 'gbtree',\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'gamma': 0.1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'eta': 0.1,\n",
        "        'eval_metric': 'auc'\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val_scaled, label=y_val)\n",
        "    print(\"\\n training data.\")\n",
        "    model = xgb.train(\n",
        "        tree_params,\n",
        "        dtrain,\n",
        "        num_boost_round=1000,\n",
        "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "        verbose_eval=100\n",
        "    )\n",
        "    y_pred_proba = model.predict(dval)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred))\n",
        "\n",
        "    print(\"\\nProcessing test data\")\n",
        "    test_processed = preprocess_data(test_df, label_encoders=label_encoders, is_training=False)\n",
        "    X_test = prepare_features(test_processed, is_training=False, agg_stats=agg_stats)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    submission = make_predictions_and_submit(\n",
        "        model,\n",
        "        X_test_scaled,\n",
        "        test_df['id'],\n",
        "        '/content/finalsub.csv'\n",
        "    )\n",
        "\n",
        "    return model, submission\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, submission = main()"
      ]
    }
  ]
}